{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. RandomForestRegressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5765\n",
      "Mean Absolute Error (MAE): 0.5616\n",
      "R² of RandomForest model: 0.0484\n",
      "Feature Importances:\n",
      "           Feature  Importance\n",
      "1  Number of Votes    0.540540\n",
      "0          Runtime    0.382493\n",
      "2            Emmys    0.076967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "data = \"./data/tvshows.csv\"\n",
    "data = pd.read_csv(data)\n",
    "\n",
    "# Select features and target\n",
    "X = data[['Runtime', 'Number of Votes', 'Emmys']]  # Input columns\n",
    "y = data['Rating']  # Output column (target)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the RandomForest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² of RandomForest model: {r2:.4f}\")\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RandomForestRegressor** model uses multiple decision trees to predict outcomes. Each tree learns how to predict the data, and the results of all trees are combined to give the final prediction. While it is very powerful in handling **non-linear relationships** between features, this model has a very low **R² value** (0.0484), indicating that the model does not explain much of the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R² = 0.0484**: This model explains only **4.84%** of the variance in the target variable, suggesting that important relationships are not being captured by the model.\n",
    "\n",
    "#### Important Features:\n",
    "- **Number of Votes**: 0.5405\n",
    "- **Runtime**: 0.3825\n",
    "- **Emmys**: 0.0770\n",
    " \n",
    "The **Number of Votes** and **Runtime** features have a significant impact on the program's ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of **RandomForestRegressor**, the following hyperparameters can be optimized:\n",
    "\n",
    "- **n_estimators**: The number of trees in the forest. Increasing this value may improve model performance, but also increases computation time. After a certain number of trees, performance improvements may plateau.\n",
    "- **max_depth**: The maximum depth of each tree. Limiting this depth can help reduce overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split a node. Increasing this value can reduce model complexity.\n",
    "- **min_samples_leaf**: The minimum number of samples required at each leaf. This helps reduce overfitting by avoiding overly detailed splits.\n",
    "- **max_features**: The maximum number of features to consider at each split. Limiting the number of features can reduce model complexity.\n",
    "- **bootstrap**: Specifies whether to use bootstrap sampling (sampling with replacement). Try both **True** and **False** to see the impact on results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5337\n",
      "Mean Absolute Error (MAE): 0.5420\n",
      "R² of Linear Regression model: 0.1190\n",
      "Coefficients and Intercept:\n",
      "Intercept: 7.758484998183272\n",
      "Feature: Runtime, Coefficient: -0.0016\n",
      "Feature: Number of Votes, Coefficient: 0.0000\n",
      "Feature: Emmys, Coefficient: 0.0132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "data = \"./data/tvshows.csv\"\n",
    "data = pd.read_csv(data)\n",
    "\n",
    "# Select features and target\n",
    "X = data[['Runtime', 'Number of Votes', 'Emmys']]  # Input columns\n",
    "y = data['Rating']  # Output column (target)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² of Linear Regression model: {r2:.4f}\")\n",
    "\n",
    "# Coefficients and Intercept\n",
    "print(\"Coefficients and Intercept:\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "for feature, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"Feature: {feature}, Coefficient: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** is a basic yet powerful model that fits a straight line to describe the relationship between the features and the target variable. This model is easy to understand and implement, but it only works well when there is a **linear relationship** between features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R² = 0.1190**: This model explains about **11.9%** of the variance, indicating that the relationship between features and the target variable is more complex than what linear regression can capture.\n",
    "\n",
    "### Coefficients:\n",
    "- **Intercept**: 7.7585\n",
    "- **Runtime**: -0.0016\n",
    "- **Number of Votes**: 0.0000\n",
    "- **Emmys**: 0.0132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression** can be optimized through the following techniques:\n",
    "\n",
    "- **Regularization**: Use **Ridge Regression** (L2 regularization) or **Lasso Regression** (L1 regularization) to avoid overfitting and improve model performance when dealing with many features.\n",
    "- **Feature Selection**: Techniques like **Recursive Feature Elimination (RFE)** or **Lasso** can help select the most important features, reducing complexity and improving performance.\n",
    "- **Polynomial Features**: If the relationship between the features and the target is not fully linear, adding **polynomial features** can improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Support Vector Regression (SVR) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5419\n",
      "Mean Absolute Error (MAE): 0.5415\n",
      "R² of SVR model: 0.1054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "data = \"./data/tvshows.csv\"\n",
    "data = pd.read_csv(data)\n",
    "\n",
    "# Select features and target\n",
    "X = data[['Runtime', 'Number of Votes', 'Emmys']]  # Input columns\n",
    "y = data['Rating']  # Output column (target)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Support Vector Regression model\n",
    "model = SVR(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² of SVR model: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Regression (SVR)** is a powerful model for handling **non-linear data**. SVR uses **support vectors** to find a regression function in a high-dimensional space, which can handle complex relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R² = 0.1054**: SVR has a similar performance to **Linear Regression**, with a low R² indicating that it does not explain much of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of **SVR**, the following hyperparameters can be optimized:\n",
    "\n",
    "- **C (Penalty Parameter)**: Increasing **C** can make the model more accurate, but it may also lead to overfitting if the value is too high.\n",
    "- **Kernel**: Try different kernels such as **linear**, **polynomial**, and **RBF** to find the most suitable one for the data.\n",
    "- **epsilon**: This parameter controls the margin of error for the model. Increasing **epsilon** can reduce model complexity, while decreasing it makes the model focus more on the data points close to the margin.\n",
    "- **gamma**: Increasing **gamma** helps the model capture more non-linear relationships, but can lead to overfitting if set too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                        | MSE      | MAE      | R²       |\n",
    "|------------------------------|----------|----------|----------|\n",
    "| **Random Forest**             | 0.5765   | 0.5616   | 0.0484   |\n",
    "| **Linear Regression**         | 0.5337   | 0.5420   | 0.1190   |\n",
    "| **Support Vector Regression** | 0.5419   | 0.5415   | 0.1054   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** performs the best in terms of **MSE** and **MAE**, but with a very low **R²**, indicating the model does not capture the relationship between the features and the target well. However, hyperparameter optimization may improve results.\n",
    "\n",
    "**Linear Regression** has a higher **R²** but performs worse in terms of **MSE** and **MAE**.\n",
    "\n",
    "**SVR** shows no significant improvement over **Linear Regression**, with similarly low **R²** values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimization** is essential for improving the performance of these models, especially when dealing with **non-linear** and **complex data**. Fine-tuning parameters can help better capture relationships and improve model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
