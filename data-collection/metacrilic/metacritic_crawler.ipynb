{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "base_url = 'https://www.metacritic.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_agents(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "def get_total_pages(url):\n",
    "    response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch the page:', url)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pagination_div = soup.find('div', {'data-testid': 'navigation-pagination'})\n",
    "    \n",
    "    return pagination_div.find_all('span', class_='c-navigationPagination_itemButtonContent')[-2].text.strip()\n",
    "\n",
    "def clean_title(title):\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"[.,'()&*]\", '', title)\n",
    "    title = title.replace(' ', '-')\n",
    "    return title\n",
    "\n",
    "def fetch_links_from_page(url, page):\n",
    "    page_url = f'{url}&page={page}'\n",
    "    response = requests.get(page_url, headers={'User-Agent': random.choice(user_agents)})\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch the page:', page_url)\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    _links = []\n",
    "    for div in soup.find_all('div', {'data-testid': 'filter-results'}):\n",
    "        link = div.find('a')['href']\n",
    "        if not link.startswith('/tv/'):\n",
    "            _title = div.find('h3', class_='c-finderProductCard_titleHeading')\n",
    "            if _title:\n",
    "                spans = _title.find_all('span')\n",
    "                if len(spans) >= 2:\n",
    "                    number = spans[0].get_text(strip=True)\n",
    "                    title = spans[1].get_text(strip=True)\n",
    "                title_text = number + ' ' + title\n",
    "                print(f\"Error link: {base_url + link} - Title: {title_text}\")\n",
    "                \n",
    "            date_span = soup.find('span', class_='u-text-uppercase')\n",
    "            if date_span:\n",
    "                date_text = date_span.get_text(strip=True)\n",
    "                year_match = re.search(r'\\d{4}', date_text)\n",
    "                if year_match:\n",
    "                    year = year_match.group(0)\n",
    "\n",
    "            alternative_link = {f'/tv/{clean_title(title)}', f'/tv/{clean_title(title)}-{year}'}\n",
    "            for alt_link in alternative_link:\n",
    "                print(f\"Trying alternative link: {base_url + alt_link}\")\n",
    "                response = requests.get(base_url + alt_link, headers={'User-Agent': random.choice(user_agents)})\n",
    "                if response.status_code == 200:\n",
    "                    link = alt_link\n",
    "                    print(f\"Alternative link for {title_text}: {base_url + link}\")\n",
    "                else:\n",
    "                    print(f\"Alternative link failed: {base_url + alt_link}\")\n",
    "        if link.startswith('/tv/'):\n",
    "            _links.append(link)\n",
    "        \n",
    "    return [base_url + link for link in _links]\n",
    "\n",
    "\n",
    "def get_show_links(url, total_pages):\n",
    "    links = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_page = {executor.submit(fetch_links_from_page, url, page): page for page in range(1, int(total_pages)+1)}\n",
    "        for future in as_completed(future_to_page):\n",
    "            page = future_to_page[future]\n",
    "            try:\n",
    "                page_links = future.result()\n",
    "                links.extend(page_links)\n",
    "            except Exception as exc:\n",
    "                print(f'Page {page} generated an exception: {exc}')\n",
    "                \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 131\n",
      "Error link: https://www.metacritic.com/ - Title: 2,017. Noughts + Crosses\n",
      "Trying alternative link: https://www.metacritic.com/tv/noughts-+-crosses\n",
      "Alternative link failed: https://www.metacritic.com/tv/noughts-+-crosses\n",
      "Trying alternative link: https://www.metacritic.com/tv/noughts-+-crosses-2020\n",
      "Alternative link for 2,017. Noughts + Crosses: https://www.metacritic.com/tv/noughts-+-crosses-2020\n",
      "Total links: 3138\n",
      "Links saved to metacritic_links.txt\n"
     ]
    }
   ],
   "source": [
    "user_agents = load_user_agents('user_agents.txt')\n",
    "\n",
    "url = f'{base_url}/browse/tv'\n",
    "total_pages = get_total_pages(url)\n",
    "print('Total pages:', total_pages)\n",
    "\n",
    "url = f'{base_url}/browse/tv?releaseYearMin=1910&releaseYearMax=2024'\n",
    "links = get_show_links(url, total_pages)\n",
    "print(\"Total links:\", len(links))\n",
    "\n",
    "filename = 'metacritic_links.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\\n\".join(links))\n",
    "print(\"Links saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# url = 'https://www.metacritic.com/tv/living-undocumented/' # 'https://www.metacritic.com/tv/the-office-uk/'\n",
    "# response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "\n",
    "# if response.status_code != 200:\n",
    "#     print('Failed to fetch the page:', url)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "# must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "# initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "# production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "# rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "# genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "\n",
    "# # scores = [div.get_text(strip=True) for div in soup.select('.c-productScoreInfo_scoreNumber')[:2]]\n",
    "# # review_counts = [div.get_text(strip=True) for div in soup.select('.c-productScoreInfo_reviewsTotal span')[:2]]\n",
    "# # metascore = scores[0] if len(scores) > 0 else None\n",
    "# # user_score = scores[1] if len(scores) > 1 else None\n",
    "# # metascore_reviews = re.search(r'\\d+', review_counts[0]).group() if len(review_counts) > 0 else None\n",
    "# # user_score_reviews = re.search(r'\\d+', review_counts[1]).group() if len(review_counts) > 1 else None\n",
    "# # score_list = []\n",
    "# # if metascore and metascore_reviews:\n",
    "# #     score_list.append(('Metascore', metascore, f'{metascore_reviews}rv'))\n",
    "# # if user_score and user_score_reviews:\n",
    "# #     score_list.append(('Userscore', user_score, f'{user_score_reviews}rv'))\n",
    "# # if not score_list:\n",
    "# #     score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "# score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "# if score_divs:\n",
    "#     metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     metascore = metascore.get_text(strip=True) if metascore else None\n",
    "#     metascore_reviews = '4*rv' if metascore == 'tbd' else None\n",
    "#     if metascore and metascore.isdigit() and 0 <= int(metascore) <= 100: metascore = int(metascore)\n",
    "#     userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     userscore = float(userscore.get_text(strip=True)) if userscore else None\n",
    "#     userscore_reviews = re.search(r'(\\d+)', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "# score_list = [('Metascore', metascore, metascore_reviews) if metascore else None,\n",
    "#               ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "# score_list = [score for score in score_list if score]\n",
    "# if not score_list:\n",
    "#     score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "    \n",
    "# awards = []\n",
    "# for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "#     award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "#     award_details = award_card.find_all('div')[1].get_text(strip=True).replace('â€¢', '').strip()\n",
    "#     awards.append((award_name, award_details))\n",
    "    \n",
    "# number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "# number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "# seasons = []\n",
    "# for season_card in soup.select('.c-seasonsModalCard'):\n",
    "#     season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "#     episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "#     episodes_count = episodes_text.split()[0]\n",
    "#     year = episodes_text.split('â€¢')[-1].strip()\n",
    "#     season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "#     seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "# print('Title:', title)\n",
    "# print('Must watch:', must_watch)\n",
    "# print(\"Initial Release Date:\", initial_release_date)\n",
    "# print(\"Production Companies:\", production_companies)\n",
    "# print(\"Rating:\", rating)\n",
    "# print(\"Genres:\", genres)\n",
    "# print(\"Score:\", score_list)\n",
    "# print(\"Awards:\", awards)\n",
    "# print(\"Number of Seasons:\", number_of_seasons)\n",
    "# print(\"Seasons:\", seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.metacritic.com/tv/living-undocumented/'\n",
    "# response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "# if response.status_code != 200:\n",
    "#     print('Failed to fetch the page:', url)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "# must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "# initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "# production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "# rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "# genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "\n",
    "# score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "# metascore, userscore, metascore_reviews, userscore_reviews = None, None, None, None\n",
    "# if score_divs:\n",
    "#     metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     metascore = metascore.get_text(strip=True) if metascore else None\n",
    "#     metascore_reviews = re.search(r'\\d+', score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(0) if score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "#     userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     userscore = userscore.get_text(strip=True) if userscore else None\n",
    "#     userscore_reviews = re.search(r'(\\d+)', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "\n",
    "# score_list = [('Metascore', str(metascore), f'{metascore_reviews}rv' if metascore_reviews else None) if metascore else None,\n",
    "#               ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "# score_list = [score for score in score_list if score]\n",
    "# if not score_list:\n",
    "#     score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "# awards = []\n",
    "# for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "#     award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "#     award_details = award_card.find_all('div')[1].get_text(strip=True).replace('â€¢', '').strip()\n",
    "#     awards.append((award_name, award_details))\n",
    "\n",
    "# number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "# number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "\n",
    "# seasons = []\n",
    "# for season_card in soup.select('.c-seasonsModalCard'):\n",
    "#     season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "#     episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "#     episodes_count = episodes_text.split()[0]\n",
    "#     year = episodes_text.split('â€¢')[-1].strip()\n",
    "#     season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "#     seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "# data = {\n",
    "#     'Title': title,\n",
    "#     'Must Watch': must_watch,\n",
    "#     'Initial Release Date': initial_release_date,\n",
    "#     'Production Companies': production_companies,\n",
    "#     'Rating': rating,\n",
    "#     'Genres': genres,\n",
    "#     'Score': score_list,\n",
    "#     'Awards': awards,\n",
    "#     'Number of Seasons': number_of_seasons,\n",
    "#     'Seasons': seasons\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame([data])\n",
    "# df.to_csv('metacritic_data.csv', index=False)\n",
    "# print('Data saved to metacritic_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_user_agents(filename):\n",
    "#     \"\"\"Load user agents from a file.\"\"\"\n",
    "#     with open(filename, 'r') as file:\n",
    "#         user_agents = [line.strip() for line in file.readlines()]\n",
    "#     return user_agents\n",
    "\n",
    "# def fetch_metacritic_data(url, user_agents):\n",
    "#     response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "#     if response.status_code != 200:\n",
    "#         print('Failed to fetch the page:', url)\n",
    "#         return None\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "#     must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "#     scores = \" - \".join([span.get_text(strip=True) for span in soup.find_all(\"span\")])\n",
    "#     production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "#     initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "#     number_of_seasons = soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True) if soup.find('span', string='Number of seasons:') else None\n",
    "#     rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "#     genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "#     awards = \" - \".join([\" \".join(award.stripped_strings).replace('â€¢', ':') for award in soup.find_all(\"div\", class_=\"c-productionAwardSummary_award\")])\n",
    "    \n",
    "#     # score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "#     # metascore, userscore, metascore_reviews, userscore_reviews = None, None, None, None\n",
    "#     # # if score_divs:\n",
    "#     # #     metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     # #     metascore = metascore.get_text(strip=True) if metascore else None\n",
    "#     # #     metascore_reviews = re.search(r'\\d+', score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(0) if score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "#     # #     userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     # #     userscore = userscore.get_text(strip=True) if userscore else None\n",
    "#     # #     userscore_reviews = re.search(r'\\d+', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "#     # for score_div in score_divs:\n",
    "#     #     metascore = score_div.select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     #     if metascore:\n",
    "#     #         metascore = metascore.get_text(strip=True)\n",
    "#     #         metascore_reviews = re.search(r'\\d+', score_div.select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(0) if score_div.select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "#     #     userscore = score_div.select_one('.c-productScoreInfo_scoreNumber span')\n",
    "#     #     if userscore:\n",
    "#     #         userscore = userscore.get_text(strip=True)\n",
    "#     #         userscore_reviews = re.search(r'\\d+', score_div.select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_div.select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "\n",
    "#     # score_list = [('Metascore', str(metascore), f'{metascore_reviews}rv' if metascore_reviews else None) if metascore else None,\n",
    "#     #               ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "#     # score_list = [score for score in score_list if score]\n",
    "#     # if not score_list:\n",
    "#     #     score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "#     # awards = []\n",
    "#     # for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "#     #     award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "#     #     award_details = award_card.find_all('div')[1].get_text(strip=True).replace('â€¢', '').strip()\n",
    "#     #     awards.append((award_name, award_details))\n",
    "    \n",
    "    \n",
    "\n",
    "#     # number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "#     # number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "\n",
    "#     # seasons = []\n",
    "#     # for season_card in soup.select('.c-seasonsModalCard'):\n",
    "#     #     season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "#     #     episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "#     #     episodes_count = episodes_text.split()[0]\n",
    "#     #     year = episodes_text.split('â€¢')[-1].strip()\n",
    "#     #     season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "#     #     seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "#     data = {\n",
    "#         'Title': title,\n",
    "#         'Must Watch': must_watch,\n",
    "#         'Scores': scores,\n",
    "#         'Production Companies': production_companies,\n",
    "#         'Initial Release Date': initial_release_date,\n",
    "#         'Number of Seasons': number_of_seasons,\n",
    "#         'Rating': rating,\n",
    "#         'Genres': genres,\n",
    "#         'Awards': awards\n",
    "#     }\n",
    "    \n",
    "#     print(url)\n",
    "#     return data\n",
    "\n",
    "# def scrape_links_from_file(filename):\n",
    "#     \"\"\"Read URLs from the file.\"\"\"\n",
    "#     with open(filename, 'r') as file:\n",
    "#         links = [line.strip() for line in file.readlines()]\n",
    "#     return links\n",
    "\n",
    "# def main():\n",
    "#     user_agents = load_user_agents('user_agents.txt')\n",
    "#     links = scrape_links_from_file('links.txt')\n",
    "#     all_data = []\n",
    "\n",
    "#     for url in links:\n",
    "#         data = fetch_metacritic_data(url, user_agents)\n",
    "#         if data:\n",
    "#             all_data.append(data)\n",
    "\n",
    "#     if all_data:\n",
    "#         df = pd.DataFrame(all_data)\n",
    "#         df.to_csv('metacritic_data.csv', index=False)\n",
    "#         print('Data saved to metacritic_data.csv')\n",
    "#     else:\n",
    "#         print('No data to save.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metacritic_data(url, user_agents):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title = soup.find('div', class_='c-productHero_title').find('h1').text.strip() if soup.find('div', class_='c-productHero_title') else None\n",
    "        must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "        _scores = [\n",
    "            span.get_text(strip=True)\n",
    "            for span in soup.find('div', class_='c-productHero_scoreInfo').find_all(\"span\")\n",
    "            if span.get_text(strip=True)\n",
    "        ]\n",
    "        scores = [\n",
    "            _scores[i] for i in range(len(_scores))\n",
    "            if i == 0 or _scores[i] != _scores[i - 1]\n",
    "        ]\n",
    "        production_companies = [\n",
    "            li.get_text(strip=True)\n",
    "            for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')\n",
    "        ] if soup.find('span', string='Production Company:') else []\n",
    "        initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "        number_of_seasons = soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True) if soup.find('span', string='Number of seasons:') else None\n",
    "        rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "        genres = list(set([\n",
    "            genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')\n",
    "        ])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "        awards = [\n",
    "            [award.find(\"div\", class_=\"g-text-bold\").get_text(strip=True), \n",
    "            award.find_all(\"div\")[1].get_text(strip=True).replace('â€¢', '')]\n",
    "            for award in soup.find_all(\"div\", class_=\"c-productionAwardSummary_award\")\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'Title': title,\n",
    "            'Must Watch': must_watch,\n",
    "            'Scores': scores,\n",
    "            'Production Companies': production_companies,\n",
    "            'Initial Release Date': initial_release_date,\n",
    "            'Number of Seasons': number_of_seasons,\n",
    "            'Rating': rating,\n",
    "            'Genres': genres,\n",
    "            'Awards': awards,\n",
    "            'Link': url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Failed to fetch data from {url}: {e}')\n",
    "        return None\n",
    "\n",
    "def scrape_links_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "def main():\n",
    "    user_agents = load_user_agents('user_agents.txt')\n",
    "    links = scrape_links_from_file('metacritic_links.txt')\n",
    "    all_data = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(fetch_metacritic_data, url, user_agents): url for url in links}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            data = future.result()\n",
    "            if data:\n",
    "                all_data.append(data)\n",
    "\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv('metacritic_data.csv', index=False)\n",
    "        print('Data saved to metacritic_data.csv')\n",
    "    else:\n",
    "        print('No data to save.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
