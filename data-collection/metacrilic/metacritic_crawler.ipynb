{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "base_url = 'https://www.metacritic.com'\n",
    "# user_agents = [\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0',\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',\n",
    "#     'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0',\n",
    "#     'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:131.0) Gecko/20100101 Firefox/131.0',\n",
    "#     'Mozilla/5.0 (X11; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0',\n",
    "#     'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pages(url):\n",
    "    response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch the page:', url)\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pagination_div = soup.find('div', {'data-testid': 'navigation-pagination'})\n",
    "    return pagination_div.find_all('span', class_='c-navigationPagination_itemButtonContent')[-2].text.strip()\n",
    "\n",
    "def fetch_links_from_page(url, page):\n",
    "    page_url = f'{url}&page={page}'\n",
    "    response = requests.get(page_url, headers={'User-Agent': random.choice(user_agents)})\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch the page:', page_url)\n",
    "        return []\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    _links = [div.find('a')['href'] for div in soup.find_all('div', {'data-testid': 'filter-results'})]\n",
    "    return [base_url + link for link in _links]\n",
    "\n",
    "def get_show_links(url, total_pages):\n",
    "    links = []\n",
    "    with ThreadPoolExecutor(max_workers = 10) as executor:\n",
    "        future_to_page = {executor.submit(fetch_links_from_page, url, page): page for page in range(1, int(total_pages)+1)}\n",
    "        for future in as_completed(future_to_page):\n",
    "            page = future_to_page[future]\n",
    "            try:\n",
    "                page_links = future.result()\n",
    "                links.extend(page_links)\n",
    "            except Exception as exc:\n",
    "                print(f'Page {page} generated an exception: {exc}')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{base_url}/browse/tv'\n",
    "total_pages = get_total_pages(url)\n",
    "print('Total pages:', total_pages)\n",
    "\n",
    "url = f'{base_url}/browse/tv?releaseYearMin=1910&releaseYearMax=2024'\n",
    "links = get_show_links(url, total_pages)\n",
    "print(\"Total links:\", len(links))\n",
    "\n",
    "filename = 'metacritic_links.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(\"\\n\".join(links))\n",
    "print(\"Links saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url = 'https://www.metacritic.com/tv/living-undocumented/' # 'https://www.metacritic.com/tv/the-office-uk/'\n",
    "response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print('Failed to fetch the page:', url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "\n",
    "# scores = [div.get_text(strip=True) for div in soup.select('.c-productScoreInfo_scoreNumber')[:2]]\n",
    "# review_counts = [div.get_text(strip=True) for div in soup.select('.c-productScoreInfo_reviewsTotal span')[:2]]\n",
    "# metascore = scores[0] if len(scores) > 0 else None\n",
    "# user_score = scores[1] if len(scores) > 1 else None\n",
    "# metascore_reviews = re.search(r'\\d+', review_counts[0]).group() if len(review_counts) > 0 else None\n",
    "# user_score_reviews = re.search(r'\\d+', review_counts[1]).group() if len(review_counts) > 1 else None\n",
    "# score_list = []\n",
    "# if metascore and metascore_reviews:\n",
    "#     score_list.append(('Metascore', metascore, f'{metascore_reviews}rv'))\n",
    "# if user_score and user_score_reviews:\n",
    "#     score_list.append(('Userscore', user_score, f'{user_score_reviews}rv'))\n",
    "# if not score_list:\n",
    "#     score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "if score_divs:\n",
    "    metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "    metascore = metascore.get_text(strip=True) if metascore else None\n",
    "    metascore_reviews = '4*rv' if metascore == 'tbd' else None\n",
    "    if metascore and metascore.isdigit() and 0 <= int(metascore) <= 100: metascore = int(metascore)\n",
    "    userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "    userscore = float(userscore.get_text(strip=True)) if userscore else None\n",
    "    userscore_reviews = re.search(r'(\\d+)', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "score_list = [('Metascore', metascore, metascore_reviews) if metascore else None,\n",
    "              ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "score_list = [score for score in score_list if score]\n",
    "if not score_list:\n",
    "    score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "    \n",
    "awards = []\n",
    "for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "    award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "    award_details = award_card.find_all('div')[1].get_text(strip=True).replace('•', '').strip()\n",
    "    awards.append((award_name, award_details))\n",
    "    \n",
    "number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "seasons = []\n",
    "for season_card in soup.select('.c-seasonsModalCard'):\n",
    "    season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "    episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "    episodes_count = episodes_text.split()[0]\n",
    "    year = episodes_text.split('•')[-1].strip()\n",
    "    season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "    seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "print('Title:', title)\n",
    "print('Must watch:', must_watch)\n",
    "print(\"Initial Release Date:\", initial_release_date)\n",
    "print(\"Production Companies:\", production_companies)\n",
    "print(\"Rating:\", rating)\n",
    "print(\"Genres:\", genres)\n",
    "print(\"Score:\", score_list)\n",
    "print(\"Awards:\", awards)\n",
    "print(\"Number of Seasons:\", number_of_seasons)\n",
    "print(\"Seasons:\", seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.metacritic.com/tv/living-undocumented/'\n",
    "response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "if response.status_code != 200:\n",
    "    print('Failed to fetch the page:', url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "\n",
    "score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "metascore, userscore, metascore_reviews, userscore_reviews = None, None, None, None\n",
    "if score_divs:\n",
    "    metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "    metascore = metascore.get_text(strip=True) if metascore else None\n",
    "    metascore_reviews = re.search(r'\\d+', score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(0) if score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "    userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "    userscore = userscore.get_text(strip=True) if userscore else None\n",
    "    userscore_reviews = re.search(r'(\\d+)', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "\n",
    "score_list = [('Metascore', str(metascore), f'{metascore_reviews}rv' if metascore_reviews else None) if metascore else None,\n",
    "              ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "score_list = [score for score in score_list if score]\n",
    "if not score_list:\n",
    "    score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "awards = []\n",
    "for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "    award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "    award_details = award_card.find_all('div')[1].get_text(strip=True).replace('•', '').strip()\n",
    "    awards.append((award_name, award_details))\n",
    "\n",
    "number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "\n",
    "seasons = []\n",
    "for season_card in soup.select('.c-seasonsModalCard'):\n",
    "    season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "    episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "    episodes_count = episodes_text.split()[0]\n",
    "    year = episodes_text.split('•')[-1].strip()\n",
    "    season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "    seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "data = {\n",
    "    'Title': title,\n",
    "    'Must Watch': must_watch,\n",
    "    'Initial Release Date': initial_release_date,\n",
    "    'Production Companies': production_companies,\n",
    "    'Rating': rating,\n",
    "    'Genres': genres,\n",
    "    'Score': score_list,\n",
    "    'Awards': awards,\n",
    "    'Number of Seasons': number_of_seasons,\n",
    "    'Seasons': seasons\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([data])\n",
    "df.to_csv('metacritic_data.csv', index=False)\n",
    "print('Data saved to metacritic_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_agents(filename):\n",
    "    \"\"\"Load user agents from a file.\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        user_agents = [line.strip() for line in file.readlines()]\n",
    "    return user_agents\n",
    "\n",
    "def fetch_metacritic_data(url, user_agents):\n",
    "    response = requests.get(url, headers={'User-Agent': random.choice(user_agents)})\n",
    "    if response.status_code != 200:\n",
    "        print('Failed to fetch the page:', url)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.find('div', class_='c-productHero_title').find('h1').text.strip()\n",
    "    must_watch = 1 if soup.find('img', class_='c-productScoreInfo_must') else 0\n",
    "    initial_release_date = soup.find('span', string='Initial Release Date:').find_next('span').get_text(strip=True) if soup.find('span', string='Initial Release Date:') else None\n",
    "    production_companies = [li.get_text(strip=True) for li in soup.find('span', string='Production Company:').find_next('ul').find_all('li')] if soup.find('span', string='Production Company:') else []\n",
    "    rating = soup.find('span', string='Rating:').find_next('span').get_text(strip=True) if soup.find('span', string='Rating:') else None\n",
    "    genres = list(set([genre.get_text(strip=True) for genre in soup.select('.c-genreList_item .c-globalButton_label')])) if soup.select('.c-genreList_item .c-globalButton_label') else None\n",
    "\n",
    "    score_divs = soup.select('.c-productScoreInfo_scoreContent')\n",
    "    metascore, userscore, metascore_reviews, userscore_reviews = None, None, None, None\n",
    "    if score_divs:\n",
    "        metascore = score_divs[0].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "        metascore = metascore.get_text(strip=True) if metascore else None\n",
    "        metascore_reviews = re.search(r'\\d+', score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(0) if score_divs[0].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "        userscore = score_divs[1].select_one('.c-productScoreInfo_scoreNumber span')\n",
    "        userscore = userscore.get_text(strip=True) if userscore else None\n",
    "        userscore_reviews = re.search(r'(\\d+)', score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span').get_text(strip=True)).group(1) if score_divs[1].select_one('.c-productScoreInfo_reviewsTotal span') else None\n",
    "\n",
    "    score_list = [('Metascore', str(metascore), f'{metascore_reviews}rv' if metascore_reviews else None) if metascore else None,\n",
    "                  ('Userscore', str(userscore), f'{userscore_reviews}rv' if userscore_reviews else None)] \n",
    "    score_list = [score for score in score_list if score]\n",
    "    if not score_list:\n",
    "        score_list = [('No scores available', 'N/A', 'N/A')]\n",
    "\n",
    "    awards = []\n",
    "    for award_card in soup.select('.c-productionAwardSummary_award'):\n",
    "        award_name = award_card.find('div', class_='g-text-bold').get_text(strip=True)\n",
    "        award_details = award_card.find_all('div')[1].get_text(strip=True).replace('•', '').strip()\n",
    "        awards.append((award_name, award_details))\n",
    "\n",
    "    number_of_seasons = re.search(r'\\d+', soup.find('span', string='Number of seasons:').find_next('span').get_text(strip=True)) if soup.find('span', string='Number of seasons:') else None\n",
    "    number_of_seasons = number_of_seasons.group(0) if number_of_seasons else None\n",
    "\n",
    "    seasons = []\n",
    "    for season_card in soup.select('.c-seasonsModalCard'):\n",
    "        season_name = season_card.find('div', class_='g-text-xsmall g-text-bold').get_text(strip=True)\n",
    "        episodes_text = season_card.find('div', class_='g-text-xsmall g-text-normal').get_text(strip=True)\n",
    "        episodes_count = episodes_text.split()[0]\n",
    "        year = episodes_text.split('•')[-1].strip()\n",
    "        season_id = f\"Ss{int(season_name.split()[-1])}\"\n",
    "        seasons.append((season_id, f'{episodes_count}eps', year))\n",
    "\n",
    "    data = {\n",
    "        'Title': title,\n",
    "        'Must Watch': must_watch,\n",
    "        'Initial Release Date': initial_release_date,\n",
    "        'Production Companies': production_companies,\n",
    "        'Rating': rating,\n",
    "        'Genres': genres,\n",
    "        'Score': score_list,\n",
    "        'Awards': awards,\n",
    "        'Number of Seasons': number_of_seasons,\n",
    "        'Seasons': seasons\n",
    "    }\n",
    "    print(url)\n",
    "    return data\n",
    "\n",
    "def scrape_links_from_file(filename):\n",
    "    \"\"\"Read URLs from the file.\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        links = [line.strip() for line in file.readlines()]\n",
    "    return links\n",
    "\n",
    "def main():\n",
    "    user_agents = load_user_agents('user_agents.txt')\n",
    "    links = scrape_links_from_file('metacritic_links.txt')\n",
    "    all_data = []\n",
    "\n",
    "    for url in links:\n",
    "        data = fetch_metacritic_data(url, user_agents)\n",
    "        if data:\n",
    "            all_data.append(data)\n",
    "\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv('metacritic_data.csv', index=False)\n",
    "        print('Data saved to metacritic_data.csv')\n",
    "    else:\n",
    "        print('No data to save.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.metacritic.com/tv/wonderland/\n",
      "https://www.metacritic.com/tv/feel-good-2020/\n",
      "https://www.metacritic.com/tv/whats-my-name-muhammad-ali/\n",
      "https://www.metacritic.com/tv/wu-tang-clan-of-mics-and-men/\n",
      "https://www.metacritic.com/tv/a-very-english-scandal/\n",
      "https://www.metacritic.com/tv/silicon-valley/\n",
      "https://www.metacritic.com/tv/insecure/\n",
      "https://www.metacritic.com/tv/los-espookys/\n",
      "https://www.metacritic.com/tv/the-crime-of-the-century/\n",
      "https://www.metacritic.com/tv/the-good-lord-bird/\n",
      "https://www.metacritic.com/tv/ramy/\n",
      "https://www.metacritic.com/tv/elvis-presley-the-searcher/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 83\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[1;32m---> 83\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_metacritic_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m     85\u001b[0m         all_data\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mfetch_metacritic_data\u001b[1;34m(url, user_agents)\u001b[0m\n\u001b[0;32m     47\u001b[0m seasons \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m season_card \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.c-seasonsModalCard\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 49\u001b[0m     season_name \u001b[38;5;241m=\u001b[39m \u001b[43mseason_card\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mg-text-xsmall g-text-bold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m     episodes_text \u001b[38;5;241m=\u001b[39m season_card\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg-text-xsmall g-text-normal\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m     episodes_count \u001b[38;5;241m=\u001b[39m episodes_text\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
